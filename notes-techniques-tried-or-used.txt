Techniques tried/used:

# Scraping
1. Basic requests
2. Scrapling dynamic fetcher (customized library on GitHub)
3. Selenium
4. Playwright (finally works)

# Parsing/processing
1. Beautiful Soup

# Algorithms
1. Recursion with constraint (to get all subpages within the base url range)
2. Iterative queue list (to solve recursion limit issue)
3. Async programming (to speed up and scrape links in parallel)
4. Round robin (to reuse open tabs and avoid overflow)

# Other major issues (solved/unsolved):
1. Make sure the page is fully loaded before extracting (sleep + scroll loop)
2. Let the browser stay open (OOP design)
3. Find hidden links through clickables, etc.
4. Save partial progress (links that have been scraped/to be visited) so that I can continue the process.
5. Potential page reuse allocation conflict caused by recursive call after redirection triggered by clicking clickable

# Small issues (solved/unsolved)
1. Links processing and parsing (join with absolute, remove query params and fragments)
2. Avoid creating duplicate tasks (more efficient in resume mode)
3. Prevent race condition with a asyncio lock to ensure number of pages is properly capped.